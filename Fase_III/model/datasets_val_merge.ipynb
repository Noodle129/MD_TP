{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge de datasets de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamento dados para Braga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>median</th>\n",
       "      <th>q1</th>\n",
       "      <th>q3</th>\n",
       "      <th>stdev</th>\n",
       "      <th>count</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-05-01T00:00:00.000Z</td>\n",
       "      <td>2.58</td>\n",
       "      <td>6.57</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.831</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.5549</td>\n",
       "      <td>-8.4067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-05-01T01:00:00.000Z</td>\n",
       "      <td>2.60</td>\n",
       "      <td>7.43</td>\n",
       "      <td>4.30</td>\n",
       "      <td>1.305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.5549</td>\n",
       "      <td>-8.4067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-05-01T02:00:00.000Z</td>\n",
       "      <td>3.45</td>\n",
       "      <td>11.95</td>\n",
       "      <td>5.21</td>\n",
       "      <td>1.839</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.5549</td>\n",
       "      <td>-8.4067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-05-01T03:00:00.000Z</td>\n",
       "      <td>2.52</td>\n",
       "      <td>8.68</td>\n",
       "      <td>4.84</td>\n",
       "      <td>1.548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.5549</td>\n",
       "      <td>-8.4067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-01T04:00:00.000Z</td>\n",
       "      <td>2.67</td>\n",
       "      <td>8.73</td>\n",
       "      <td>4.53</td>\n",
       "      <td>1.351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.5549</td>\n",
       "      <td>-8.4067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date   min    max  median     q1  q3  stdev  count  \\\n",
       "0  2023-05-01T00:00:00.000Z  2.58   6.57    3.72  0.831 NaN    NaN    NaN   \n",
       "1  2023-05-01T01:00:00.000Z  2.60   7.43    4.30  1.305 NaN    NaN    NaN   \n",
       "2  2023-05-01T02:00:00.000Z  3.45  11.95    5.21  1.839 NaN    NaN    NaN   \n",
       "3  2023-05-01T03:00:00.000Z  2.52   8.68    4.84  1.548 NaN    NaN    NaN   \n",
       "4  2023-05-01T04:00:00.000Z  2.67   8.73    4.53  1.351 NaN    NaN    NaN   \n",
       "\n",
       "   latitude  longitude  \n",
       "0   41.5549    -8.4067  \n",
       "1   41.5549    -8.4067  \n",
       "2   41.5549    -8.4067  \n",
       "3   41.5549    -8.4067  \n",
       "4   41.5549    -8.4067  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset from the file\n",
    "df_pm10 = pd.read_csv('datasets/validation_data/braga_data_pm10_val.csv')\n",
    "\n",
    "# Add latitude and longitude columns\n",
    "df_pm10['latitude'] = 41.5549\n",
    "df_pm10['longitude'] = -8.4067\n",
    "\n",
    "# Save the updated dataset back to the file\n",
    "df_pm10.to_csv('datasets/validation_data/braga_data_pm10_val_complete.csv', index=False)\n",
    "\n",
    "# Print the first 5 rows of the dataset\n",
    "\n",
    "df_pm10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>median</th>\n",
       "      <th>q1</th>\n",
       "      <th>q3</th>\n",
       "      <th>stdev</th>\n",
       "      <th>count</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-05-01T00:00:00.000Z</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2.67</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.281</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.5549</td>\n",
       "      <td>-8.4067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-05-01T01:00:00.000Z</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.5549</td>\n",
       "      <td>-8.4067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-05-01T02:00:00.000Z</td>\n",
       "      <td>1.58</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2.58</td>\n",
       "      <td>0.485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.5549</td>\n",
       "      <td>-8.4067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-05-01T03:00:00.000Z</td>\n",
       "      <td>1.67</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2.57</td>\n",
       "      <td>0.377</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.5549</td>\n",
       "      <td>-8.4067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-01T04:00:00.000Z</td>\n",
       "      <td>1.85</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2.38</td>\n",
       "      <td>0.337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.5549</td>\n",
       "      <td>-8.4067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date   min   max  median     q1  q3  stdev  count  \\\n",
       "0  2023-05-01T00:00:00.000Z  1.42  2.67    1.79  0.281 NaN    NaN    NaN   \n",
       "1  2023-05-01T01:00:00.000Z  1.48  2.45    2.04  0.289 NaN    NaN    NaN   \n",
       "2  2023-05-01T02:00:00.000Z  1.58  3.30    2.58  0.485 NaN    NaN    NaN   \n",
       "3  2023-05-01T03:00:00.000Z  1.67  3.20    2.57  0.377 NaN    NaN    NaN   \n",
       "4  2023-05-01T04:00:00.000Z  1.85  3.45    2.38  0.337 NaN    NaN    NaN   \n",
       "\n",
       "   latitude  longitude  \n",
       "0   41.5549    -8.4067  \n",
       "1   41.5549    -8.4067  \n",
       "2   41.5549    -8.4067  \n",
       "3   41.5549    -8.4067  \n",
       "4   41.5549    -8.4067  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pm25 = pd.read_csv('datasets/validation_data/braga_data_pm25_val.csv')\n",
    "\n",
    "df_pm25['latitude'] = 41.5549\n",
    "df_pm25['longitude'] = -8.4067\n",
    "\n",
    "df_pm25.to_csv('datasets/validation_data/braga_data_pm25_val_complete.csv', index=False)\n",
    "\n",
    "df_pm25.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the columns we need\n",
    "\n",
    "df_updated = df_pm10[['date', 'latitude', 'longitude', 'median']]\n",
    "\n",
    "df_updated.columns = ['date', 'latitude', 'longitude', 'pm10']\n",
    "\n",
    "df_updated.head()\n",
    "\n",
    "df_updated.to_csv('datasets/validation_data/braga_data_pm10_val_complete.csv', index=False)\n",
    "\n",
    "df_updated = df_pm25[['date', 'latitude', 'longitude', 'median']]\n",
    "\n",
    "df_updated.columns = ['date', 'latitude', 'longitude', 'pm25']\n",
    "\n",
    "df_updated.head()\n",
    "\n",
    "df_updated.to_csv('datasets/validation_data/braga_data_pm25_val_complete.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two datasets into one\n",
    "\n",
    "df_pm10 = pd.read_csv('datasets/validation_data/braga_data_pm10_val_complete.csv')\n",
    "df_pm25 = pd.read_csv('datasets/validation_data/braga_data_pm25_val_complete.csv')\n",
    "\n",
    "df_merged = pd.merge(df_pm10, df_pm25, on=['date', 'latitude', 'longitude'])\n",
    "\n",
    "df_merged.head()\n",
    "\n",
    "df_merged.to_csv('datasets/validation_data/braga_41.5549_-8.4067_val.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamento dados Ponte de Lima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pm1 = pd.read_csv('datasets/validation_data/braga_ponte_lima_pm1_val.csv')\n",
    "df_pm10 = pd.read_csv('datasets/validation_data/braga_ponte_lima_pm10_val.csv')\n",
    "df_pm25 = pd.read_csv('datasets/validation_data/braga_ponte_lima_pm25_val.csv')\n",
    "\n",
    "# Add latitude and longitude columns\n",
    "\n",
    "# Select only the columns we need\n",
    "\n",
    "df_pm1 = df_pm1[['date', 'median']]\n",
    "df_pm10 = df_pm10[['date', 'median']]\n",
    "df_pm25 = df_pm25[['date', 'median']]\n",
    "\n",
    "# Rename the columns\n",
    "\n",
    "df_pm1.columns = ['date', 'pm1']\n",
    "df_pm10.columns = ['date', 'pm10']\n",
    "df_pm25.columns = ['date', 'pm25']\n",
    "\n",
    "# Add latitude and longitude columns\n",
    "\n",
    "df_pm1['latitude'] = 41.653315\n",
    "df_pm1['longitude'] = -8.587790\n",
    "\n",
    "df_pm10['latitude'] = 41.653315\n",
    "df_pm10['longitude'] = -8.587790\n",
    "\n",
    "df_pm25['latitude'] = 41.653315\n",
    "df_pm25['longitude'] = -8.587790\n",
    "\n",
    "# Merge the three datasets into one\n",
    "\n",
    "df_merged = pd.merge(df_pm1, df_pm10, on=['date', 'latitude', 'longitude'])\n",
    "df_merged = pd.merge(df_merged, df_pm25, on=['date', 'latitude', 'longitude'])\n",
    "\n",
    "df_merged.head()\n",
    "\n",
    "df_merged.to_csv('datasets/validation_data/ponte_lima_41.7675_-8.5823_val.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamento de Dados do OpenAQ para Braga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from OpenAQ\n",
    "import requests\n",
    "import time\n",
    "\n",
    "location_id = \"7191\"\n",
    "city = \"Braga\"\n",
    "\n",
    "# change date_from and date_to to get different time periods\n",
    "# query for 2015-2018 and 2018-2020 and 2020-2023, separated because of the large amount of data\n",
    "# in case of 'connection timeout' error, try again with a smaller time period or query for a single period istead of 3\n",
    "\n",
    "url = f\"https://api.openaq.org/v2/measurements?format=csv&date_from=2023-05-01&date_to=2023-05-12&limit=100000&page=1&offset=0&sort=desc&radius=1000&country_id=PT&country=PT&city={city}&location_id={location_id}&order_by=datetime\"\n",
    "\n",
    "headers = {\"accept\": \"application/json\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "with open(f\"{city}_{location_id}_2023.csv\", \"w\") as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "with open(f\"datasets/validation_data/{city}_{location_id}_2020_2023.csv\", \"w\") as f:\n",
    "    f.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Não há dados para este período"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Datasets into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_braga = pd.read_csv('datasets/validation_data/braga_41.5549_-8.4067_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ponlim = pd.read_csv('datasets/validation_data/ponte_lima_41.7675_-8.5823_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47915/1015182262.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_final = df_braga.append(df_braga)\n",
      "/tmp/ipykernel_47915/1015182262.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_final = df_final.append(df_ponlim)\n"
     ]
    }
   ],
   "source": [
    "# Merge the datasets\n",
    "\n",
    "df_final = df_braga.append(df_braga)\n",
    "\n",
    "df_final = df_final.append(df_ponlim)\n",
    "\n",
    "df_final.head()\n",
    "\n",
    "# to csv \n",
    "\n",
    "df_final.to_csv('datasets/validation_data/datasetsfinal/dataset_final_braga_ponte_lima_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the df_final date column into two columns: date and time\n",
    "\n",
    "df_final = pd.read_csv('datasets/validation_data/datasetsfinal/dataset_final_braga_ponte_lima_val.csv')\n",
    "\n",
    "df_final['date'] = pd.to_datetime(df_final['date'])\n",
    "\n",
    "# Create a new 'date_only' column\n",
    "df_final['date_only'] = df_final['date'].dt.date\n",
    "\n",
    "# Create a new 'time' column\n",
    "df_final['time'] = df_final['date'].dt.time\n",
    "\n",
    "# You can drop the original 'date' column if you want\n",
    "df_final = df_final.drop(columns='date')\n",
    "\n",
    "df_final.head()\n",
    "\n",
    "df_final.to_csv('datasets/validation_data/datasetsfinal/dataset_final_braga_ponte_lima_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the datasets meteo.csv and FINALDATASET_v2.csv based on date and latitude and longitude\n",
    "\n",
    "df_meteo = pd.read_csv('datasets/validation_data/Braga_2023-05-01_to _2023-05-12_meteo_val.csv')\n",
    "\n",
    "df_total = pd.read_csv('datasets/validation_data/datasetsfinal/dataset_final_braga_ponte_lima_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# find the common dates between the two datasets\n",
    "# meteo dataset has datetime as '2023-05-01T00:00:00'\n",
    "\n",
    "df_meteo['datetime'] = pd.to_datetime(df_meteo['datetime'], format='%Y-%m-%dT%H:%M:%S')\n",
    "df_total['date_only'] = pd.to_datetime(df_total['date_only'])\n",
    "\n",
    "dates1 = set(df_meteo['datetime'])\n",
    "dates2 = set(df_total['date_only'])\n",
    "\n",
    "# Find the common dates\n",
    "common_dates = dates1.intersection(dates2)\n",
    "\n",
    "print(len(common_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>pm10</th>\n",
       "      <th>pm25</th>\n",
       "      <th>pm1</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.5549</td>\n",
       "      <td>-8.4067</td>\n",
       "      <td>3.72</td>\n",
       "      <td>1.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.5549</td>\n",
       "      <td>-8.4067</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.5549</td>\n",
       "      <td>-8.4067</td>\n",
       "      <td>5.21</td>\n",
       "      <td>2.58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-01 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.5549</td>\n",
       "      <td>-8.4067</td>\n",
       "      <td>4.84</td>\n",
       "      <td>2.57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.5549</td>\n",
       "      <td>-8.4067</td>\n",
       "      <td>4.53</td>\n",
       "      <td>2.38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-01 04:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   latitude  longitude  pm10  pm25  pm1            datetime\n",
       "0   41.5549    -8.4067  3.72  1.79  NaN 2023-05-01 00:00:00\n",
       "1   41.5549    -8.4067  4.30  2.04  NaN 2023-05-01 01:00:00\n",
       "2   41.5549    -8.4067  5.21  2.58  NaN 2023-05-01 02:00:00\n",
       "3   41.5549    -8.4067  4.84  2.57  NaN 2023-05-01 03:00:00\n",
       "4   41.5549    -8.4067  4.53  2.38  NaN 2023-05-01 04:00:00\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge date_only and time columns into one column datetime\n",
    "\n",
    "df_total['datetime'] = pd.to_datetime(df_total['date_only'].astype(str) + ' ' + df_total['time'].astype(str))\n",
    "\n",
    "# drop the date_only and time columns\n",
    "\n",
    "df_total = df_total.drop(columns=['date_only', 'time'])\n",
    "\n",
    "\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the datasets on datetime (if they do not exist put null values)\n",
    "# Only want to keep the rows that are in both datasets\n",
    "\n",
    "# drop duplicates in df_meteo and df_total\n",
    "df_meteo = df_meteo.drop_duplicates(subset=['datetime'])\n",
    "df_total = df_total.drop_duplicates(subset=['datetime'])\n",
    "\n",
    "# merge the two datasets\n",
    "df_final = pd.merge(df_total, df_meteo, how='inner', on='datetime')\n",
    "\n",
    "df_final.to_csv('datasets/validation_data/datasetsfinal/dataset_final_braga_ponte_lima_merged_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the dataset final, if the day is the same the atributes tempmax,tempmin,temp,feelslike,dew,humidity,severerisk,precip,snow,windspeed,sealevelpressure,cloudcover,visibility,solarradiation,solarenergy,uvindex are the same\n",
    "\n",
    "df = pd.read_csv('datasets/validation_data/datasetsfinal/dataset_final_braga_ponte_lima_merged_val.csv')\n",
    "\n",
    "# Combine 'date' and 'time' into a single datetime column to ensure the filling happens in chronological order\n",
    "df = df.sort_values('datetime')\n",
    "\n",
    "\n",
    "# List of columns to fill\n",
    "columns_to_fill = ['temp', 'feelslike', 'dew', 'humidity', 'severerisk', \n",
    "                   'precip', 'snow', 'windspeed', 'sealevelpressure', 'cloudcover', 'visibility', \n",
    "                   'solarradiation', 'solarenergy', 'uvindex']\n",
    "\n",
    "# Apply forward fill\n",
    "for col in columns_to_fill:\n",
    "    df[col] = df[col].fillna(method='ffill')\n",
    "\n",
    "# You can also backward fill for any remaining nulls if needed\n",
    "for col in columns_to_fill:\n",
    "    df[col] = df[col].fillna(method='bfill')\n",
    "\n",
    "\n",
    "df.to_csv('datasets/validation_data/datasetsfinal/FINALDATASET.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate AQI for each row\n",
    "def calculateAQI(data):\n",
    "\n",
    "    # define breakpoints\n",
    "    breakpoints = [\n",
    "        {\n",
    "            \"pollutant\": \"no2\",\n",
    "            \"conc\": [0, 50, 100, 200, 400, 1000],\n",
    "            \"aqi\": [0, 50, 100, 150, 200, 300, 400],\n",
    "        },\n",
    "        {\n",
    "            \"pollutant\": \"pm10\",\n",
    "            \"conc\": [0, 20, 40, 70, 100, 200],\n",
    "            \"aqi\": [0, 50, 100, 150, 200, 300, 400],\n",
    "        },\n",
    "        {\n",
    "            \"pollutant\": \"pm25\",\n",
    "            \"conc\": [0, 12, 35.4, 55.4, 150.4, 250.4],\n",
    "            \"aqi\": [0, 50, 100, 150, 200, 300, 400],\n",
    "        },\n",
    "        {\n",
    "            \"pollutant\": \"pm1\",\n",
    "            \"conc\": [0, 12, 35.4, 55.4, 150.4, 250.4],\n",
    "            \"aqi\": [0, 50, 100, 150, 200, 300, 400],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # calculate AQI for each pollutant\n",
    "    def aqiForConcentration(pollutant, conc):\n",
    "        bp = next(bp for bp in breakpoints if bp[\"pollutant\"] == pollutant)\n",
    "        i = next((i for i, c in enumerate(bp[\"conc\"]) if c > conc), len(bp[\"conc\"]) - 1)\n",
    "        cLow = bp[\"conc\"][i]\n",
    "        cHigh = bp[\"conc\"][i] if i == len(bp[\"conc\"]) - 1 else bp[\"conc\"][i + 1]\n",
    "        aqiLow = bp[\"aqi\"][i]\n",
    "        aqiHigh = bp[\"aqi\"][i] if i == len(bp[\"conc\"]) - 1 else bp[\"aqi\"][i + 1]    \n",
    "        if cHigh == cLow:  # Avoid ZeroDivisionError\n",
    "            return aqiHigh  # or aqiLow, they should be equal in this case\n",
    "        else:\n",
    "            return round(((aqiHigh - aqiLow) / (cHigh - cLow)) * (conc - cLow) + aqiLow)\n",
    "\n",
    "    # calculate AQI for each row\n",
    "    for index, row in data.iterrows():\n",
    "        aqi = None\n",
    "        for pollutant in [\"pm10\", \"pm1\", \"pm25\"]:\n",
    "            if row[pollutant] is not None:\n",
    "                aqi = aqiForConcentration(pollutant, row[pollutant])\n",
    "                break\n",
    "        data.at[index, \"AQI\"] = aqi\n",
    "    \n",
    "    return data\n",
    "\n",
    "# calculate AQI for each row\n",
    "\n",
    "df = calculateAQI(df)\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.to_csv('datasets/validation_data/datasetsfinal/dataset_final_val.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "md",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
